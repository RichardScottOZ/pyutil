{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use boto3 to train and inference with script-mode XGBoost\n",
    "\n",
    "This notebook shows how to train an XGBoost in script mode, then perform the inference using batch transform, using boto3 instead of SageMaker SDK. A typical use-case is targeted at Lambda. A Lambda function is limited to 250 MB uncompressed, and it's not sufficient to cram SageMaker SDK with other ML libraries such as pandas + sklearn. In this case, the Lambda can use the pandas+sklearn layer (which is already almost 250 MB uncompressed), then interact with SageMaker via boto3.\n",
    "\n",
    "The high-level steps are as follows:\n",
    "\n",
    "- create an entrypoint script, with the same convention and requirements as sklearn or mxnet entrypoint script.\n",
    "- package entrypoint script (+dependencies) to S3.\n",
    "- in the create training job request, add a few **hyperparameters** to specify the entrypoint script name & package.\n",
    "- in the create model request, add a few **environment variables** to specify the entrypoint script name & package. The environment variables ensure that endpoint or batch transform are able to locate & invoke the entrypoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package script to S3\n",
    "\n",
    "To recap, with SageMaker SDK, each fit() will cause the SDK to package and upload the entrypoint script (+dependencies) from local disk to S3. With boto3, we need to package our script (+dependencies) to S3. This is ok since typically Lambda is used in a production or operational pipeline, thus there's a clear need for a stable, controlled entrypoint package deployed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package entrypoint script (+optional dependencies)\n",
    "!GZIP=--best tar -czvf sourcedir.tar.gz sm_entry_point.py\n",
    "\n",
    "# Upload entrypoint package to S3\n",
    "%env ENTRYPOINT_SRC=s3://bucket/path/to/src/\n",
    "!aws s3 cp sourcedir.tar.gz $ENTRYPOINT_SRC\n",
    "\n",
    "# Synchronize env var to Python variable for later usage\n",
    "import os\n",
    "entrypoint_src = os.environ['ENTRYPOINT_SRC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Create a training request with specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm_client = sess.client('sagemaker')\n",
    "\n",
    "# Must use xgb container version 0.90\n",
    "image = '783357654285.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3'\n",
    "\n",
    "train_job_name = 'my_training_job_12572'\n",
    "sm_role = \"CHANGE_ME: SM_EXECUTION_ROLE\"\n",
    "train_channel = 's3://bucket/path/to/data/train/'\n",
    "valid_channel = 's3://bucket/path/to/data/valid/'\n",
    "train_output = 's3://bucket/path/to/sagemaker_train_output/'   # NOTE: SageMaker will output to $train_output/$sm_jobname/output/model.tar.gz.\n",
    "\n",
    "entrypoint_name = 'sm_entry_point.py'\n",
    "entrypoint_src = 's3://bucket/path/to/src/sourcedir.tar.gz...'\n",
    "hyperparameters = {\n",
    "    'sagemaker_program' = entrypoint_name,            # Mandatory\n",
    "    'sagemaker_submit_directory' = entrypoint_src,    # Mandatory\n",
    "    'sagemaker_container_log_level': '20',            # Optional\n",
    "    'sagemaker_enable_cloudwatch_metrics': 'false',   # Optional\n",
    "\n",
    "    # Additional hyperparameter supported by entrypoint\n",
    "    # Additional hyperparameter supported by entrypoint\n",
    "    # Additional hyperparameter supported by entrypoint\n",
    "    # ...\n",
    "}\n",
    "\n",
    "train_param = {\n",
    "        \"TrainingJobName\": train_job_name,\n",
    "        \"AlgorithmSpecification\": {\n",
    "          \"TrainingImage\": image,\n",
    "          \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        \"RoleArn\": sm_role,\n",
    "        \"StoppingCondition\": {\n",
    "          \"MaxRuntimeInSeconds\": 86400\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "          \"InstanceCount\": \"1\",\n",
    "          \"InstanceType\": \"ml.m5.large\",\n",
    "          \"VolumeSizeInGB\": 30\n",
    "        },\n",
    "        \"HyperParameters\": hyperparameters,\n",
    "        \"InputDataConfig\": [\n",
    "          {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "              \"S3DataSource\": {\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": train_channel\n",
    "              }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"InputMode\": \"File\"\n",
    "          },\n",
    "          {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "              \"S3DataSource\": {\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri.$\": valid_channel\n",
    "              }\n",
    "            },\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"InputMode\": \"File\"\n",
    "          }\n",
    "        ],\n",
    "        \"OutputDataConfig\": {\n",
    "          \"S3OutputPath\": train_output\n",
    "        }\n",
    "      }\n",
    "\n",
    "sm_client.create_training_job(train_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model\n",
    "\n",
    "We specify the entrypoint settings of a model by setting a few environment variables.\n",
    "\n",
    "The following example probes the training job to automatically deduce what the environment variables that a model must use. In addition, the example also pull a few settings (e.g., image url) from the training job -- essentially, treating the training job's metadata as the single-source, centralized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'my_model'\n",
    "\n",
    "# Attempt to delete existing model which has the same name as the request.\n",
    "train_job = sm_client.describe_training_job(TrainingJobName=train_jobname)\n",
    "try:\n",
    "    response = sm_client.delete_model(ModelName=model_name)\n",
    "except ClientError as e:\n",
    "    if 'Could not find model' in e.response['Error']['Message']:\n",
    "        pass        # No previous model, so we're okay to proceed further\n",
    "    else:\n",
    "        raise e     # Oh no, something terrible happens, so let's escalate the error.\n",
    "else:\n",
    "    print(f'Deleted pre-existing model {model_name}')\n",
    "\n",
    "# The environment variables to indicate entry point.\n",
    "# Will automatically synchronize from the training job.\n",
    "env = {}\n",
    "if 'sagemaker_program' in train_job['HyperParameters']:\n",
    "    env['SAGEMAKER_PROGRAM'] = train_job['HyperParameters']['sagemaker_program']\n",
    "    env['SAGEMAKER_SUBMIT_DIRECTORY'] = train_job['HyperParameters']['sagemaker_submit_directory']\n",
    "    env['SAGEMAKER_CONTAINER_LOG_LEVEL'] = '20'\n",
    "    env['SAGEMAKER_ENABLE_CLOUDWATCH_METRICS'] = 'False'\n",
    "\n",
    "# Create an SM model.\n",
    "# Will automatically synchronize a settings from the training job.\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            'Image': train_job['AlgorithmSpecification']['TrainingImage'],\n",
    "            'ModelDataUrl': train_job['ModelArtifacts']['S3ModelArtifacts'],\n",
    "            'Environment': env,\n",
    "        },\n",
    "    ],\n",
    "    ExecutionRoleArn=sm_role,      # NOTE: can use a different role than training\n",
    "    EnableNetworkIsolation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Nothing special on the inference side. Simply create an endpoint or a batch transform job using the model, and SageMaker will correctly use the entry point specified by the model's environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips on XGBoost Hyperparameters\n",
    "\n",
    "There're a few differences in hyperparameters depending on whether the XGBoost container runs in algorithm vs script mode.\n",
    "\n",
    "Renamed hyperparameters:\n",
    "\n",
    "Algorithm Mode | Script Mode\n",
    "--- | ---\n",
    "`num_round` | `n_estimators`\n",
    "`eta` | `learning_rate`\n",
    "\n",
    "\n",
    "In addition, the `objective` hyperparameter also differs in the accepted values:\n",
    "\n",
    "- algorithm mode: see the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html). Supported values include `reg:linear`, `reg:logistic`, `multi:softmax`. Note that `reg:squarederror` is not supported, and possibly a few others.\n",
    "- script mode: see [supported values](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
